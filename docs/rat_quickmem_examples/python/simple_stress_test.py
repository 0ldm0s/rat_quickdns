#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ RatQuickMem Python ç®€åŒ–å‹åŠ›æµ‹è¯•

æ— éœ€é¢å¤–ä¾èµ–çš„è½»é‡çº§å‹åŠ›æµ‹è¯•å¥—ä»¶
æµ‹è¯•åœºæ™¯ï¼š
1. å¹¶å‘ç¼–è§£ç æµ‹è¯•
2. å¤§æ•°æ®å¤„ç†æµ‹è¯•
3. æ‰¹é‡æ“ä½œæµ‹è¯•
4. å†…å­˜æ± éªŒè¯æµ‹è¯•
"""

import rat_quickmem_py as rat_quickmem
import threading
import time
import random
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

class SimpleStressTest:
    def __init__(self):
        self.results = {}
        self.errors = []
        self.start_time = None
        
    def log(self, message: str):
        """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")
        
    def generate_test_data(self, size_kb: int) -> bytes:
        """ç”ŸæˆæŒ‡å®šå¤§å°çš„æµ‹è¯•æ•°æ®"""
        return bytes(random.getrandbits(8) for _ in range(size_kb * 1024))
        
    def test_concurrent_operations(self, num_threads: int = 20, operations_per_thread: int = 50):
        """ğŸ”¥ å¹¶å‘æ“ä½œå‹åŠ›æµ‹è¯•"""
        self.log(f"å¼€å§‹å¹¶å‘æµ‹è¯•: {num_threads} çº¿ç¨‹ x {operations_per_thread} æ“ä½œ")
        
        def worker(thread_id: int) -> Dict[str, Any]:
            results = {"success": 0, "errors": 0, "total_time": 0}
            
            for i in range(operations_per_thread):
                try:
                    start = time.time()
                    
                    # éšæœºå¤§å°çš„æ•°æ® (1KB - 50KB)
                    data_size = random.randint(1, 50)
                    test_data = self.generate_test_data(data_size)
                    
                    # ç¼–ç è§£ç 
                    encoded = rat_quickmem.encode(test_data)
                    decoded = rat_quickmem.decode(encoded)
                    
                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    if decoded != test_data:
                        raise ValueError("æ•°æ®éªŒè¯å¤±è´¥")
                    
                    results["total_time"] += time.time() - start
                    results["success"] += 1
                    
                except Exception as e:
                    results["errors"] += 1
                    self.errors.append(f"Thread {thread_id}, Op {i}: {str(e)}")
                    
            return results
        
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(worker, i) for i in range(num_threads)]
            thread_results = [future.result() for future in as_completed(futures)]
        
        end_time = time.time()
        
        # ç»Ÿè®¡ç»“æœ
        total_success = sum(r["success"] for r in thread_results)
        total_errors = sum(r["errors"] for r in thread_results)
        total_ops = total_success + total_errors
        total_time = sum(r["total_time"] for r in thread_results)
        
        self.results["concurrent_operations"] = {
            "total_operations": total_ops,
            "success_count": total_success,
            "error_count": total_errors,
            "success_rate": total_success / total_ops * 100 if total_ops > 0 else 0,
            "ops_per_second": total_ops / (end_time - start_time),
            "avg_operation_time_ms": (total_time / total_ops * 1000) if total_ops > 0 else 0,
            "duration_seconds": end_time - start_time
        }
        
        self.log(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {total_success}/{total_ops} æˆåŠŸ, "
                f"{self.results['concurrent_operations']['ops_per_second']:.1f} ops/s")
    
    def test_large_data_processing(self, data_sizes_mb: List[int] = [1, 2, 5, 10]):
        """ğŸ“¦ å¤§æ•°æ®å¤„ç†æµ‹è¯•"""
        self.log(f"å¼€å§‹å¤§æ•°æ®å¤„ç†æµ‹è¯•: {data_sizes_mb} MB")
        
        large_data_results = []
        
        for size_mb in data_sizes_mb:
            try:
                self.log(f"  æµ‹è¯• {size_mb}MB æ•°æ®...")
                
                # ç”Ÿæˆå¤§æ•°æ®
                large_data = self.generate_test_data(size_mb * 1024)
                
                # ç¼–ç æµ‹è¯•
                encode_start = time.time()
                encoded = rat_quickmem.encode(large_data)
                encode_time = time.time() - encode_start
                
                # è§£ç æµ‹è¯•
                decode_start = time.time()
                decoded = rat_quickmem.decode(encoded)
                decode_time = time.time() - decode_start
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if decoded != large_data:
                    raise ValueError("å¤§æ•°æ®éªŒè¯å¤±è´¥")
                
                result = {
                    "size_mb": size_mb,
                    "original_size": len(large_data),
                    "encoded_size": len(encoded),
                    "encode_time_ms": encode_time * 1000,
                    "decode_time_ms": decode_time * 1000,
                    "encode_speed_mbps": size_mb / encode_time if encode_time > 0 else 0,
                    "decode_speed_mbps": size_mb / decode_time if decode_time > 0 else 0,
                    "size_overhead_bytes": len(encoded) - len(large_data),
                    "overhead_percentage": (len(encoded) - len(large_data)) / len(large_data) * 100
                }
                
                large_data_results.append(result)
                self.log(f"    âœ… {size_mb}MB: ç¼–ç  {encode_time*1000:.1f}ms ({result['encode_speed_mbps']:.1f} MB/s), "
                        f"è§£ç  {decode_time*1000:.1f}ms ({result['decode_speed_mbps']:.1f} MB/s)")
                self.log(f"    ğŸ“Š å¼€é”€: {result['size_overhead_bytes']} å­—èŠ‚ ({result['overhead_percentage']:.3f}%)")
                
                # æ¸…ç†å†…å­˜
                del large_data, encoded, decoded
                gc.collect()
                
            except Exception as e:
                self.log(f"    âŒ {size_mb}MB æµ‹è¯•å¤±è´¥: {str(e)}")
                self.errors.append(f"Large data {size_mb}MB: {str(e)}")
        
        self.results["large_data_processing"] = large_data_results
        self.log("âœ… å¤§æ•°æ®å¤„ç†æµ‹è¯•å®Œæˆ")
    
    def test_batch_operations(self):
        """ğŸ“Š æ‰¹é‡æ“ä½œæµ‹è¯•"""
        self.log("å¼€å§‹æ‰¹é‡æ“ä½œæµ‹è¯•")
        
        batch_sizes = [10, 50, 100, 500, 1000, 2000, 5000, 8000, 9000, 9500, 9900, 9999]
        batch_results = []
        
        for batch_size in batch_sizes:
            try:
                self.log(f"  æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
                
                # ç”Ÿæˆæ‰¹é‡æ•°æ®
                batch_data = []
                for i in range(batch_size):
                    data = {
                        "id": i,
                        "value": f"test_data_{i}_{random.randint(1000, 9999)}",
                        "timestamp": time.time(),
                        "random_bytes": self.generate_test_data(1)[:100].hex()  # 100å­—èŠ‚éšæœºæ•°æ®
                    }
                    batch_data.append(data)
                
                start_time = time.time()
                
                # æ‰¹é‡ç¼–ç 
                encoded_batch = rat_quickmem.encode_batch(batch_data)
                encode_time = time.time() - start_time
                
                # æ‰¹é‡è§£ç 
                decode_start = time.time()
                decoded_batch = rat_quickmem.decode_batch(encoded_batch)
                decode_time = time.time() - decode_start
                
                total_time = time.time() - start_time
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if len(decoded_batch) != batch_size:
                    raise ValueError(f"æ‰¹é‡å¤§å°ä¸åŒ¹é…: æœŸæœ› {batch_size}, å®é™… {len(decoded_batch)}")
                
                if decoded_batch[0]["id"] != 0 or decoded_batch[-1]["id"] != batch_size - 1:
                    raise ValueError("æ‰¹é‡æ•°æ®é¡ºåºé”™è¯¯")
                
                result = {
                    "batch_size": batch_size,
                    "success": True,
                    "encode_time_ms": encode_time * 1000,
                    "decode_time_ms": decode_time * 1000,
                    "total_time_ms": total_time * 1000,
                    "items_per_second": batch_size / total_time if total_time > 0 else 0,
                    "encode_items_per_second": batch_size / encode_time if encode_time > 0 else 0,
                    "decode_items_per_second": batch_size / decode_time if decode_time > 0 else 0
                }
                
                batch_results.append(result)
                self.log(f"    âœ… {batch_size} é¡¹: {result['total_time_ms']:.1f}ms "
                        f"({result['items_per_second']:.0f} items/s)")
                
            except Exception as e:
                result = {
                    "batch_size": batch_size,
                    "success": False,
                    "error": str(e)
                }
                batch_results.append(result)
                self.log(f"    âŒ {batch_size} é¡¹å¤±è´¥: {str(e)}")
                
                # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œè®°å½•å¹¶ç»§ç»­æµ‹è¯•æ›´å°çš„æ‰¹é‡
                if "CountExceeded" in str(e) or "exceeded" in str(e).lower():
                    self.log(f"    ğŸ“ è¾¾åˆ°æ‰¹é‡æ“ä½œé™åˆ¶")
        
        self.results["batch_operations"] = batch_results
        self.log("âœ… æ‰¹é‡æ“ä½œæµ‹è¯•å®Œæˆ")
    
    def test_memory_pool_verification(self, iterations: int = 500):
        """ğŸŠ å†…å­˜æ± éªŒè¯æµ‹è¯•"""
        self.log(f"å¼€å§‹å†…å­˜æ± éªŒè¯æµ‹è¯•: {iterations} æ¬¡è¿­ä»£")
        
        pool_stats_samples = []
        
        # è·å–åˆå§‹æ± çŠ¶æ€
        initial_stats = rat_quickmem.get_pool_stats()
        pool_stats_samples.append({
            "iteration": 0,
            "small_buffers": initial_stats.small_buffers,
            "medium_buffers": initial_stats.medium_buffers,
            "large_buffers": initial_stats.large_buffers,
            "total_buffers": initial_stats.total_buffers
        })
        
        self.log(f"  åˆå§‹æ± çŠ¶æ€: {initial_stats.total_buffers} ä¸ªç¼“å†²åŒº")
        
        for i in range(iterations):
            try:
                # éšæœºå¤§å°æ•°æ® (1KB - 500KB)
                data_size = random.randint(1, 500)
                test_data = self.generate_test_data(data_size)
                
                # ç¼–è§£ç æ“ä½œ
                encoded = rat_quickmem.encode(test_data)
                decoded = rat_quickmem.decode(encoded)
                
                if decoded != test_data:
                    raise ValueError(f"æ•°æ®éªŒè¯å¤±è´¥ at iteration {i}")
                
                # æ¯100æ¬¡è®°å½•æ± çŠ¶æ€
                if (i + 1) % 100 == 0:
                    pool_stats = rat_quickmem.get_pool_stats()
                    pool_stats_samples.append({
                        "iteration": i + 1,
                        "small_buffers": pool_stats.small_buffers,
                        "medium_buffers": pool_stats.medium_buffers,
                        "large_buffers": pool_stats.large_buffers,
                        "total_buffers": pool_stats.total_buffers
                    })
                    
                    self.log(f"  è¿›åº¦: {i+1}/{iterations}, æ± ç¼“å†²åŒº: {pool_stats.total_buffers}")
                
            except Exception as e:
                self.errors.append(f"Memory pool iteration {i}: {str(e)}")
        
        # è·å–æœ€ç»ˆæ± çŠ¶æ€
        final_stats = rat_quickmem.get_pool_stats()
        
        self.results["memory_pool_verification"] = {
            "iterations": iterations,
            "initial_pool_stats": {
                "small_buffers": initial_stats.small_buffers,
                "medium_buffers": initial_stats.medium_buffers,
                "large_buffers": initial_stats.large_buffers,
                "total_buffers": initial_stats.total_buffers
            },
            "final_pool_stats": {
                "small_buffers": final_stats.small_buffers,
                "medium_buffers": final_stats.medium_buffers,
                "large_buffers": final_stats.large_buffers,
                "total_buffers": final_stats.total_buffers
            },
            "pool_growth": final_stats.total_buffers - initial_stats.total_buffers,
            "pool_stats_samples": pool_stats_samples
        }
        
        self.log(f"âœ… å†…å­˜æ± æµ‹è¯•å®Œæˆ: ç¼“å†²åŒºä» {initial_stats.total_buffers} å¢é•¿åˆ° {final_stats.total_buffers}")
    
    def test_mixed_workload(self, duration_seconds: int = 60):
        """ğŸ”€ æ··åˆè´Ÿè½½æµ‹è¯•"""
        self.log(f"å¼€å§‹æ··åˆè´Ÿè½½æµ‹è¯•: {duration_seconds} ç§’")
        
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        operation_counts = {
            "single_encode": 0,
            "batch_encode": 0,
            "large_data": 0,
            "errors": 0
        }
        
        while time.time() < end_time:
            try:
                # éšæœºé€‰æ‹©æ“ä½œç±»å‹
                operation_type = random.choice(["single", "single", "batch", "large"])  # å•ä¸ªæ“ä½œæƒé‡æ›´é«˜
                
                if operation_type == "single":
                    # å•ä¸ªç¼–è§£ç 
                    data_size = random.randint(1, 100)  # 1KB - 100KB
                    test_data = self.generate_test_data(data_size)
                    encoded = rat_quickmem.encode(test_data)
                    decoded = rat_quickmem.decode(encoded)
                    
                    if decoded != test_data:
                        raise ValueError("å•ä¸ªç¼–è§£ç éªŒè¯å¤±è´¥")
                    
                    operation_counts["single_encode"] += 1
                    
                elif operation_type == "batch":
                    # å°æ‰¹é‡æ“ä½œ
                    batch_size = random.randint(5, 50)
                    batch_data = [f"batch_item_{i}_{random.randint(1000, 9999)}" for i in range(batch_size)]
                    
                    encoded_batch = rat_quickmem.encode_batch(batch_data)
                    decoded_batch = rat_quickmem.decode_batch(encoded_batch)
                    
                    if decoded_batch != batch_data:
                        raise ValueError("æ‰¹é‡æ“ä½œéªŒè¯å¤±è´¥")
                    
                    operation_counts["batch_encode"] += 1
                    
                elif operation_type == "large":
                    # å¤§æ•°æ®æ“ä½œ
                    data_size = random.randint(100, 1000)  # 100KB - 1MB
                    large_data = self.generate_test_data(data_size)
                    
                    encoded = rat_quickmem.encode(large_data)
                    decoded = rat_quickmem.decode(encoded)
                    
                    if decoded != large_data:
                        raise ValueError("å¤§æ•°æ®æ“ä½œéªŒè¯å¤±è´¥")
                    
                    operation_counts["large_data"] += 1
                
            except Exception as e:
                operation_counts["errors"] += 1
                self.errors.append(f"Mixed workload: {str(e)}")
        
        actual_duration = time.time() - start_time
        total_operations = sum(operation_counts.values()) - operation_counts["errors"]
        
        self.results["mixed_workload"] = {
            "duration_seconds": actual_duration,
            "operation_counts": operation_counts,
            "total_operations": total_operations,
            "ops_per_second": total_operations / actual_duration if actual_duration > 0 else 0,
            "error_rate": operation_counts["errors"] / (total_operations + operation_counts["errors"]) * 100 if total_operations > 0 else 0
        }
        
        self.log(f"âœ… æ··åˆè´Ÿè½½æµ‹è¯•å®Œæˆ: {total_operations} æ“ä½œ, "
                f"{self.results['mixed_workload']['ops_per_second']:.1f} ops/s, "
                f"é”™è¯¯ç‡ {self.results['mixed_workload']['error_rate']:.2f}%")
    
    def run_all_tests(self):
        """ğŸš€ è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        self.log("="*60)
        self.log("ğŸš€ å¼€å§‹ RatQuickMem Python ç®€åŒ–å‹åŠ›æµ‹è¯•")
        self.log("="*60)
        
        self.start_time = time.time()
        
        try:
            # 1. å¹¶å‘æ“ä½œæµ‹è¯•
            self.test_concurrent_operations(num_threads=10, operations_per_thread=30)
            
            # 2. å¤§æ•°æ®å¤„ç†æµ‹è¯•
            self.test_large_data_processing([1, 2, 5])
            
            # 3. æ‰¹é‡æ“ä½œæµ‹è¯•
            self.test_batch_operations()
            
            # 4. å†…å­˜æ± éªŒè¯æµ‹è¯•
            self.test_memory_pool_verification(iterations=300)
            
            # 5. æ··åˆè´Ÿè½½æµ‹è¯•
            self.test_mixed_workload(duration_seconds=30)
            
        except KeyboardInterrupt:
            self.log("âš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.log(f"âŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œé”™è¯¯: {str(e)}")
            self.errors.append(f"Test suite error: {str(e)}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ğŸ“‹ ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        
        self.log("\n" + "="*60)
        self.log("ğŸ“‹ ç®€åŒ–å‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        self.log("="*60)
        
        # æ€»ä½“ç»Ÿè®¡
        self.log(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.1f} ç§’")
        self.log(f"âŒ æ€»é”™è¯¯æ•°: {len(self.errors)}")
        
        # å„é¡¹æµ‹è¯•ç»“æœ
        for test_name, result in self.results.items():
            self.log(f"\nğŸ” {test_name.replace('_', ' ').title()}:")
            
            if test_name == "concurrent_operations":
                self.log(f"  - æˆåŠŸç‡: {result['success_rate']:.1f}%")
                self.log(f"  - ååé‡: {result['ops_per_second']:.1f} ops/s")
                self.log(f"  - å¹³å‡å»¶è¿Ÿ: {result['avg_operation_time_ms']:.2f} ms")
                
            elif test_name == "large_data_processing":
                for item in result:
                    self.log(f"  - {item['size_mb']}MB: ç¼–ç  {item['encode_speed_mbps']:.1f} MB/s, "
                            f"è§£ç  {item['decode_speed_mbps']:.1f} MB/s")
                    self.log(f"    å¼€é”€: {item['size_overhead_bytes']} å­—èŠ‚ ({item['overhead_percentage']:.3f}%)")
                        
            elif test_name == "batch_operations":
                successful_batches = [r for r in result if r.get('success', False)]
                failed_batches = [r for r in result if not r.get('success', False)]
                
                if successful_batches:
                    max_batch = max(r['batch_size'] for r in successful_batches)
                    self.log(f"  - æœ€å¤§æˆåŠŸæ‰¹é‡: {max_batch}")
                    
                    # æ˜¾ç¤ºæ€§èƒ½æœ€å¥½çš„å‡ ä¸ªæ‰¹é‡å¤§å°
                    top_performers = sorted(successful_batches, key=lambda x: x['items_per_second'], reverse=True)[:3]
                    for perf in top_performers:
                        self.log(f"  - {perf['batch_size']} é¡¹: {perf['items_per_second']:.0f} items/s")
                
                if failed_batches:
                    self.log(f"  - å¤±è´¥çš„æ‰¹é‡å¤§å°: {[r['batch_size'] for r in failed_batches]}")
                    
            elif test_name == "memory_pool_verification":
                self.log(f"  - ç¼“å†²åŒºå¢é•¿: {result['initial_pool_stats']['total_buffers']} â†’ {result['final_pool_stats']['total_buffers']} (+{result['pool_growth']})")
                self.log(f"  - æœ€ç»ˆæ± çŠ¶æ€: å°å‹ {result['final_pool_stats']['small_buffers']}, "
                        f"ä¸­å‹ {result['final_pool_stats']['medium_buffers']}, "
                        f"å¤§å‹ {result['final_pool_stats']['large_buffers']}")
                
            elif test_name == "mixed_workload":
                self.log(f"  - è¿è¡Œæ—¶é—´: {result['duration_seconds']:.1f} ç§’")
                self.log(f"  - æ€»æ“ä½œæ•°: {result['total_operations']}")
                self.log(f"  - ååé‡: {result['ops_per_second']:.1f} ops/s")
                self.log(f"  - é”™è¯¯ç‡: {result['error_rate']:.2f}%")
                self.log(f"  - æ“ä½œåˆ†å¸ƒ: å•ä¸ª {result['operation_counts']['single_encode']}, "
                        f"æ‰¹é‡ {result['operation_counts']['batch_encode']}, "
                        f"å¤§æ•°æ® {result['operation_counts']['large_data']}")
        
        # é”™è¯¯æ±‡æ€»
        if self.errors:
            self.log(f"\nâŒ é”™è¯¯è¯¦æƒ… (å‰5ä¸ª):")
            for error in self.errors[:5]:
                self.log(f"  - {error}")
            if len(self.errors) > 5:
                self.log(f"  ... è¿˜æœ‰ {len(self.errors) - 5} ä¸ªé”™è¯¯")
        
        # æ€§èƒ½è¯„çº§
        self.log(f"\nğŸ† æ€§èƒ½è¯„çº§:")
        
        # å¹¶å‘æ€§èƒ½è¯„çº§
        if "concurrent_operations" in self.results:
            concurrent_result = self.results["concurrent_operations"]
            if concurrent_result["success_rate"] >= 99 and concurrent_result["ops_per_second"] >= 500:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€")
            elif concurrent_result["success_rate"] >= 95 and concurrent_result["ops_per_second"] >= 200:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸğŸŒŸ è‰¯å¥½")
            else:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸ ä¸€èˆ¬")
        
        # æ‰¹é‡å¤„ç†è¯„çº§
        if "batch_operations" in self.results:
            batch_result = self.results["batch_operations"]
            successful_batches = [r for r in batch_result if r.get('success', False)]
            if successful_batches:
                max_batch = max(r['batch_size'] for r in successful_batches)
                if max_batch >= 5000:
                    self.log("  - æ‰¹é‡å¤„ç†: ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€")
                elif max_batch >= 1000:
                    self.log("  - æ‰¹é‡å¤„ç†: ğŸŒŸğŸŒŸ è‰¯å¥½")
                else:
                    self.log("  - æ‰¹é‡å¤„ç†: ğŸŒŸ ä¸€èˆ¬")
        
        # ç¨³å®šæ€§è¯„çº§
        if "mixed_workload" in self.results:
            mixed_result = self.results["mixed_workload"]
            if mixed_result["error_rate"] <= 0.1 and mixed_result["ops_per_second"] >= 100:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€")
            elif mixed_result["error_rate"] <= 1.0 and mixed_result["ops_per_second"] >= 50:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸğŸŒŸ è‰¯å¥½")
            else:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸ ä¸€èˆ¬")
        
        self.log("\nâœ… ç®€åŒ–å‹åŠ›æµ‹è¯•å®Œæˆï¼")
        self.log("\nğŸ’¡ æç¤º:")
        self.log("  - å¦‚éœ€æ›´è¯¦ç»†çš„æµ‹è¯•ï¼Œè¯·å®‰è£… psutil å¹¶è¿è¡Œ stress_test.py")
        self.log("  - æµ‹è¯•ç»“æœä»…ä¾›å‚è€ƒï¼Œå®é™…æ€§èƒ½å¯èƒ½å› ç¯å¢ƒè€Œå¼‚")
        self.log("  - å¦‚å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯è¯¦æƒ…å¹¶è°ƒæ•´æµ‹è¯•å‚æ•°")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RatQuickMem Python ç®€åŒ–å‹åŠ›æµ‹è¯•")
    print("æ— éœ€é¢å¤–ä¾èµ–çš„è½»é‡çº§æµ‹è¯•å¥—ä»¶")
    print("æŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­æµ‹è¯•\n")
    
    # è¿è¡Œæµ‹è¯•
    runner = SimpleStressTest()
    runner.run_all_tests()

if __name__ == "__main__":
    main()