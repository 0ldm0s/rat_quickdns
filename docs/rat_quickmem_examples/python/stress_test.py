#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ RatQuickMem Python å‹åŠ›æµ‹è¯•å¥—ä»¶

æµ‹è¯•åœºæ™¯ï¼š
1. é«˜å¹¶å‘ç¼–è§£ç å‹åŠ›æµ‹è¯•
2. å¤§æ•°æ®é‡å¤„ç†å‹åŠ›æµ‹è¯•
3. å†…å­˜æ± å‹åŠ›æµ‹è¯•
4. é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•
5. æ‰¹é‡æ“ä½œæé™æµ‹è¯•
6. æ··åˆè´Ÿè½½å‹åŠ›æµ‹è¯•
"""

import rat_quickmem_py as rat_quickmem
import threading
import time
import random
import gc
import psutil
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import json

class StressTestRunner:
    def __init__(self):
        self.results = {}
        self.errors = []
        self.start_time = None
        self.process = psutil.Process(os.getpid())
        
    def log(self, message: str):
        """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—è¾“å‡º"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")
        
    def get_memory_usage(self) -> Dict[str, float]:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = self.process.memory_info()
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # ç‰©ç†å†…å­˜
            "vms_mb": memory_info.vms / 1024 / 1024,  # è™šæ‹Ÿå†…å­˜
        }
        
    def generate_test_data(self, size_kb: int) -> bytes:
        """ç”ŸæˆæŒ‡å®šå¤§å°çš„æµ‹è¯•æ•°æ®"""
        return bytes(random.getrandbits(8) for _ in range(size_kb * 1024))
        
    def test_concurrent_encoding(self, num_threads: int = 50, operations_per_thread: int = 100):
        """ğŸ”¥ é«˜å¹¶å‘ç¼–ç å‹åŠ›æµ‹è¯•"""
        self.log(f"å¼€å§‹é«˜å¹¶å‘ç¼–ç æµ‹è¯•: {num_threads} çº¿ç¨‹ x {operations_per_thread} æ“ä½œ")
        
        def encode_worker(thread_id: int) -> Dict[str, Any]:
            results = {"success": 0, "errors": 0, "total_time": 0}
            
            for i in range(operations_per_thread):
                try:
                    start = time.time()
                    
                    # éšæœºå¤§å°çš„æ•°æ® (1KB - 100KB)
                    data_size = random.randint(1, 100)
                    test_data = self.generate_test_data(data_size)
                    
                    # ç¼–ç 
                    encoded = rat_quickmem.encode(test_data)
                    
                    # éªŒè¯
                    decoded = rat_quickmem.decode(encoded)
                    assert decoded == test_data
                    
                    results["total_time"] += time.time() - start
                    results["success"] += 1
                    
                except Exception as e:
                    results["errors"] += 1
                    self.errors.append(f"Thread {thread_id}, Op {i}: {str(e)}")
                    
            return results
        
        start_time = time.time()
        start_memory = self.get_memory_usage()
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(encode_worker, i) for i in range(num_threads)]
            thread_results = [future.result() for future in as_completed(futures)]
        
        end_time = time.time()
        end_memory = self.get_memory_usage()
        
        # ç»Ÿè®¡ç»“æœ
        total_success = sum(r["success"] for r in thread_results)
        total_errors = sum(r["errors"] for r in thread_results)
        total_ops = total_success + total_errors
        avg_time = sum(r["total_time"] for r in thread_results) / total_ops if total_ops > 0 else 0
        
        self.results["concurrent_encoding"] = {
            "total_operations": total_ops,
            "success_rate": total_success / total_ops * 100 if total_ops > 0 else 0,
            "ops_per_second": total_ops / (end_time - start_time),
            "avg_operation_time_ms": avg_time * 1000,
            "memory_delta_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
            "duration_seconds": end_time - start_time
        }
        
        self.log(f"âœ… å¹¶å‘ç¼–ç æµ‹è¯•å®Œæˆ: {total_success}/{total_ops} æˆåŠŸ, "
                f"{self.results['concurrent_encoding']['ops_per_second']:.1f} ops/s")
    
    def test_large_data_processing(self, data_sizes_mb: List[int] = [1, 5, 10, 20]):
        """ğŸ“¦ å¤§æ•°æ®é‡å¤„ç†å‹åŠ›æµ‹è¯•"""
        self.log(f"å¼€å§‹å¤§æ•°æ®å¤„ç†æµ‹è¯•: {data_sizes_mb} MB")
        
        large_data_results = []
        
        for size_mb in data_sizes_mb:
            try:
                self.log(f"  æµ‹è¯• {size_mb}MB æ•°æ®...")
                
                # ç”Ÿæˆå¤§æ•°æ®
                large_data = self.generate_test_data(size_mb * 1024)
                
                start_memory = self.get_memory_usage()
                
                # ç¼–ç æµ‹è¯•
                encode_start = time.time()
                encoded = rat_quickmem.encode(large_data)
                encode_time = time.time() - encode_start
                
                # è§£ç æµ‹è¯•
                decode_start = time.time()
                decoded = rat_quickmem.decode(encoded)
                decode_time = time.time() - decode_start
                
                end_memory = self.get_memory_usage()
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                assert decoded == large_data
                
                result = {
                    "size_mb": size_mb,
                    "encode_time_ms": encode_time * 1000,
                    "decode_time_ms": decode_time * 1000,
                    "encode_speed_mbps": size_mb / encode_time,
                    "decode_speed_mbps": size_mb / decode_time,
                    "compression_ratio": len(encoded) / len(large_data),
                    "memory_peak_mb": end_memory["rss_mb"] - start_memory["rss_mb"]
                }
                
                large_data_results.append(result)
                self.log(f"    âœ… {size_mb}MB: ç¼–ç  {encode_time*1000:.1f}ms, "
                        f"è§£ç  {decode_time*1000:.1f}ms")
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                del large_data, encoded, decoded
                gc.collect()
                
            except Exception as e:
                self.log(f"    âŒ {size_mb}MB æµ‹è¯•å¤±è´¥: {str(e)}")
                self.errors.append(f"Large data {size_mb}MB: {str(e)}")
        
        self.results["large_data_processing"] = large_data_results
        self.log("âœ… å¤§æ•°æ®å¤„ç†æµ‹è¯•å®Œæˆ")
    
    def test_memory_pool_stress(self, iterations: int = 1000):
        """ğŸŠ å†…å­˜æ± å‹åŠ›æµ‹è¯•"""
        self.log(f"å¼€å§‹å†…å­˜æ± å‹åŠ›æµ‹è¯•: {iterations} æ¬¡è¿­ä»£")
        
        start_memory = self.get_memory_usage()
        pool_stats_history = []
        
        for i in range(iterations):
            try:
                # éšæœºå¤§å°æ•°æ® (1KB - 1MB)
                data_size = random.randint(1, 1024)
                test_data = self.generate_test_data(data_size)
                
                # ç¼–è§£ç 
                encoded = rat_quickmem.encode(test_data)
                decoded = rat_quickmem.decode(encoded)
                assert decoded == test_data
                
                # æ¯100æ¬¡è®°å½•æ± çŠ¶æ€
                if i % 100 == 0:
                    pool_stats = rat_quickmem.get_pool_stats()
                    pool_stats_history.append({
                        "iteration": i,
                        "small_buffers": pool_stats.small_buffers,
                        "medium_buffers": pool_stats.medium_buffers,
                        "large_buffers": pool_stats.large_buffers,
                        "total_buffers": pool_stats.total_buffers,
                        "memory_mb": self.get_memory_usage()["rss_mb"]
                    })
                    
                    if i % 200 == 0:
                        self.log(f"  è¿›åº¦: {i}/{iterations}, æ± ç¼“å†²åŒº: {pool_stats.total_buffers}")
                
            except Exception as e:
                self.errors.append(f"Memory pool iteration {i}: {str(e)}")
        
        end_memory = self.get_memory_usage()
        final_pool_stats = rat_quickmem.get_pool_stats()
        
        self.results["memory_pool_stress"] = {
            "iterations": iterations,
            "final_pool_stats": {
                "small_buffers": final_pool_stats.small_buffers,
                "medium_buffers": final_pool_stats.medium_buffers,
                "large_buffers": final_pool_stats.large_buffers,
                "total_buffers": final_pool_stats.total_buffers
            },
            "memory_delta_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
            "pool_stats_history": pool_stats_history
        }
        
        self.log(f"âœ… å†…å­˜æ± æµ‹è¯•å®Œæˆ: æœ€ç»ˆç¼“å†²åŒºæ•°é‡ {final_pool_stats.total_buffers}")
    
    def test_batch_operations_limit(self):
        """ğŸ“Š æ‰¹é‡æ“ä½œæé™æµ‹è¯•"""
        self.log("å¼€å§‹æ‰¹é‡æ“ä½œæé™æµ‹è¯•")
        
        batch_sizes = [100, 500, 1000, 2000, 5000, 8000, 9000, 9500, 9900, 9999]
        batch_results = []
        
        for batch_size in batch_sizes:
            try:
                self.log(f"  æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
                
                # ç”Ÿæˆæ‰¹é‡æ•°æ®
                batch_data = []
                for i in range(batch_size):
                    data = {
                        "id": i,
                        "data": self.generate_test_data(1).hex(),  # 1KB æ•°æ®è½¬hex
                        "timestamp": time.time()
                    }
                    batch_data.append(data)
                
                start_time = time.time()
                
                # æ‰¹é‡ç¼–ç 
                encoded_batch = rat_quickmem.encode_batch(batch_data)
                
                # æ‰¹é‡è§£ç 
                decoded_batch = rat_quickmem.decode_batch(encoded_batch)
                
                end_time = time.time()
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                assert len(decoded_batch) == batch_size
                assert decoded_batch[0]["id"] == 0
                assert decoded_batch[-1]["id"] == batch_size - 1
                
                result = {
                    "batch_size": batch_size,
                    "success": True,
                    "total_time_ms": (end_time - start_time) * 1000,
                    "items_per_second": batch_size / (end_time - start_time)
                }
                
                batch_results.append(result)
                self.log(f"    âœ… {batch_size} é¡¹: {result['total_time_ms']:.1f}ms")
                
            except Exception as e:
                result = {
                    "batch_size": batch_size,
                    "success": False,
                    "error": str(e)
                }
                batch_results.append(result)
                self.log(f"    âŒ {batch_size} é¡¹å¤±è´¥: {str(e)}")
                
                # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•
                if "CountExceeded" in str(e) or "exceeded" in str(e).lower():
                    self.log(f"    è¾¾åˆ°æ‰¹é‡æ“ä½œé™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                    break
        
        self.results["batch_operations_limit"] = batch_results
        self.log("âœ… æ‰¹é‡æ“ä½œæé™æµ‹è¯•å®Œæˆ")
    
    def test_long_running_stability(self, duration_minutes: int = 5):
        """â° é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
        self.log(f"å¼€å§‹é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•: {duration_minutes} åˆ†é’Ÿ")
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        operation_count = 0
        error_count = 0
        memory_samples = []
        
        while time.time() < end_time:
            try:
                # éšæœºæ“ä½œç±»å‹
                operation_type = random.choice(["single", "batch", "large"])
                
                if operation_type == "single":
                    # å•ä¸ªç¼–è§£ç 
                    data = self.generate_test_data(random.randint(1, 100))
                    encoded = rat_quickmem.encode(data)
                    decoded = rat_quickmem.decode(encoded)
                    assert decoded == data
                    
                elif operation_type == "batch":
                    # å°æ‰¹é‡æ“ä½œ
                    batch_size = random.randint(10, 100)
                    batch_data = [f"item_{i}" for i in range(batch_size)]
                    encoded_batch = rat_quickmem.encode_batch(batch_data)
                    decoded_batch = rat_quickmem.decode_batch(encoded_batch)
                    assert decoded_batch == batch_data
                    
                elif operation_type == "large":
                    # å¤§æ•°æ®æ“ä½œ
                    large_data = self.generate_test_data(random.randint(100, 1000))
                    encoded = rat_quickmem.encode(large_data)
                    decoded = rat_quickmem.decode(encoded)
                    assert decoded == large_data
                
                operation_count += 1
                
                # æ¯1000æ¬¡æ“ä½œè®°å½•å†…å­˜
                if operation_count % 1000 == 0:
                    memory_info = self.get_memory_usage()
                    memory_samples.append({
                        "operation": operation_count,
                        "time": time.time() - start_time,
                        "memory_mb": memory_info["rss_mb"]
                    })
                    
                    elapsed_minutes = (time.time() - start_time) / 60
                    self.log(f"  è¿è¡Œ {elapsed_minutes:.1f}åˆ†é’Ÿ: {operation_count} æ“ä½œ, "
                            f"å†…å­˜ {memory_info['rss_mb']:.1f}MB")
                
            except Exception as e:
                error_count += 1
                self.errors.append(f"Long running test operation {operation_count}: {str(e)}")
        
        total_time = time.time() - start_time
        
        self.results["long_running_stability"] = {
            "duration_minutes": total_time / 60,
            "total_operations": operation_count,
            "error_count": error_count,
            "success_rate": (operation_count - error_count) / operation_count * 100 if operation_count > 0 else 0,
            "ops_per_minute": operation_count / (total_time / 60),
            "memory_samples": memory_samples,
            "memory_growth_mb": memory_samples[-1]["memory_mb"] - memory_samples[0]["memory_mb"] if memory_samples else 0
        }
        
        self.log(f"âœ… é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•å®Œæˆ: {operation_count} æ“ä½œ, "
                f"æˆåŠŸç‡ {self.results['long_running_stability']['success_rate']:.1f}%")
    
    def run_all_tests(self):
        """ğŸš€ è¿è¡Œæ‰€æœ‰å‹åŠ›æµ‹è¯•"""
        self.log("="*60)
        self.log("ğŸš€ å¼€å§‹ RatQuickMem Python å‹åŠ›æµ‹è¯•å¥—ä»¶")
        self.log("="*60)
        
        self.start_time = time.time()
        initial_memory = self.get_memory_usage()
        
        try:
            # 1. é«˜å¹¶å‘ç¼–ç æµ‹è¯•
            self.test_concurrent_encoding(num_threads=20, operations_per_thread=50)
            
            # 2. å¤§æ•°æ®å¤„ç†æµ‹è¯•
            self.test_large_data_processing([1, 5, 10])
            
            # 3. å†…å­˜æ± å‹åŠ›æµ‹è¯•
            self.test_memory_pool_stress(iterations=500)
            
            # 4. æ‰¹é‡æ“ä½œæé™æµ‹è¯•
            self.test_batch_operations_limit()
            
            # 5. é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯• (è¾ƒçŸ­æ—¶é—´)
            self.test_long_running_stability(duration_minutes=2)
            
        except KeyboardInterrupt:
            self.log("âš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            self.log(f"âŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œé”™è¯¯: {str(e)}")
            self.errors.append(f"Test suite error: {str(e)}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report(initial_memory)
    
    def generate_report(self, initial_memory: Dict[str, float]):
        """ğŸ“‹ ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š"""
        total_time = time.time() - self.start_time
        final_memory = self.get_memory_usage()
        
        self.log("\n" + "="*60)
        self.log("ğŸ“‹ å‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        self.log("="*60)
        
        # æ€»ä½“ç»Ÿè®¡
        self.log(f"â±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.1f} ç§’")
        self.log(f"ğŸ’¾ å†…å­˜ä½¿ç”¨å˜åŒ–: {final_memory['rss_mb'] - initial_memory['rss_mb']:+.1f} MB")
        self.log(f"âŒ æ€»é”™è¯¯æ•°: {len(self.errors)}")
        
        # å„é¡¹æµ‹è¯•ç»“æœ
        for test_name, result in self.results.items():
            self.log(f"\nğŸ” {test_name.replace('_', ' ').title()}:")
            
            if test_name == "concurrent_encoding":
                self.log(f"  - æˆåŠŸç‡: {result['success_rate']:.1f}%")
                self.log(f"  - ååé‡: {result['ops_per_second']:.1f} ops/s")
                self.log(f"  - å¹³å‡å»¶è¿Ÿ: {result['avg_operation_time_ms']:.2f} ms")
                
            elif test_name == "large_data_processing":
                for item in result:
                    if item.get('encode_speed_mbps'):
                        self.log(f"  - {item['size_mb']}MB: ç¼–ç  {item['encode_speed_mbps']:.1f} MB/s, "
                                f"è§£ç  {item['decode_speed_mbps']:.1f} MB/s")
                        
            elif test_name == "memory_pool_stress":
                self.log(f"  - æœ€ç»ˆç¼“å†²åŒº: {result['final_pool_stats']['total_buffers']}")
                self.log(f"  - å†…å­˜å¢é•¿: {result['memory_delta_mb']:+.1f} MB")
                
            elif test_name == "batch_operations_limit":
                successful_batches = [r for r in result if r.get('success', False)]
                if successful_batches:
                    max_batch = max(r['batch_size'] for r in successful_batches)
                    self.log(f"  - æœ€å¤§æ‰¹é‡å¤§å°: {max_batch}")
                    
            elif test_name == "long_running_stability":
                self.log(f"  - è¿è¡Œæ—¶é—´: {result['duration_minutes']:.1f} åˆ†é’Ÿ")
                self.log(f"  - æ€»æ“ä½œæ•°: {result['total_operations']}")
                self.log(f"  - æˆåŠŸç‡: {result['success_rate']:.1f}%")
                self.log(f"  - å†…å­˜å¢é•¿: {result['memory_growth_mb']:+.1f} MB")
        
        # é”™è¯¯æ±‡æ€»
        if self.errors:
            self.log(f"\nâŒ é”™è¯¯è¯¦æƒ… (å‰10ä¸ª):")
            for error in self.errors[:10]:
                self.log(f"  - {error}")
            if len(self.errors) > 10:
                self.log(f"  ... è¿˜æœ‰ {len(self.errors) - 10} ä¸ªé”™è¯¯")
        
        # æ€§èƒ½è¯„çº§
        self.log(f"\nğŸ† æ€§èƒ½è¯„çº§:")
        
        # å¹¶å‘æ€§èƒ½è¯„çº§
        if "concurrent_encoding" in self.results:
            concurrent_result = self.results["concurrent_encoding"]
            if concurrent_result["success_rate"] >= 99 and concurrent_result["ops_per_second"] >= 1000:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€")
            elif concurrent_result["success_rate"] >= 95 and concurrent_result["ops_per_second"] >= 500:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸğŸŒŸ è‰¯å¥½")
            else:
                self.log("  - å¹¶å‘æ€§èƒ½: ğŸŒŸ ä¸€èˆ¬")
        
        # ç¨³å®šæ€§è¯„çº§
        if "long_running_stability" in self.results:
            stability_result = self.results["long_running_stability"]
            if stability_result["success_rate"] >= 99.9 and stability_result["memory_growth_mb"] < 10:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸğŸŒŸğŸŒŸ ä¼˜ç§€")
            elif stability_result["success_rate"] >= 99 and stability_result["memory_growth_mb"] < 50:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸğŸŒŸ è‰¯å¥½")
            else:
                self.log("  - ç¨³å®šæ€§: ğŸŒŸ ä¸€èˆ¬")
        
        self.log("\nâœ… å‹åŠ›æµ‹è¯•å®Œæˆï¼")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        report_file = "stress_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_time_seconds": total_time,
                "memory_delta_mb": final_memory['rss_mb'] - initial_memory['rss_mb'],
                "error_count": len(self.errors),
                "results": self.results,
                "errors": self.errors
            }, f, indent=2, ensure_ascii=False)
        
        self.log(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RatQuickMem Python å‹åŠ›æµ‹è¯•")
    print("æŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­æµ‹è¯•\n")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import psutil
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–: pip install psutil")
        return
    
    # è¿è¡Œæµ‹è¯•
    runner = StressTestRunner()
    runner.run_all_tests()

if __name__ == "__main__":
    main()